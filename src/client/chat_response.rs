use super::chat_input::Message;
use serde::{Deserialize, Serialize};

/// Represents the response from the chat API call.
///
/// Fields:
/// - `id`: Unique identifier for the response.
/// - `object`: The type of object returned (e.g., "chat.completion").
/// - `created`: Timestamp indicating when the response was created.
/// - `model`: The model used to generate the response.
/// - `usage`: Information about token usage for the request.
/// - `choices`: List of choices generated by the model.
#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct ChatResponse {
    pub id: String,
    pub object: String,
    pub created: i64,
    pub model: String,
    pub usage: Usage,
    pub choices: Vec<Choice>,
}

/// Represents the usage information in the chat API response.
///
/// Fields:
/// - `prompt_tokens`: The number of tokens in the input prompt.
/// - `completion_tokens`: The number of tokens in the generated completion.
/// - `total_tokens`: The total number of tokens used (prompt + completion).
#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct Usage {
    pub prompt_tokens: i64,
    pub completion_tokens: i64,
    pub total_tokens: i64,
}

/// Represents a choice in the chat API response.
///
/// Fields:
/// - `message`: The message generated by the model.
/// - `finish_reason`: The reason why the completion finished (e.g., "stop", "length").
#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct Choice {
    pub message: Message,
    pub finish_reason: String,
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::models::Role;

    #[test]
    fn test_chat_response_struct() {
        let usage = Usage {
            prompt_tokens: 10,
            completion_tokens: 20,
            total_tokens: 30,
        };

        let message = Message {
            role: Role::Assistant,
            content: "This is a test message.".to_string(),
        };

        let choice = Choice {
            message: message.clone(),
            finish_reason: "stop".to_string(),
        };

        let chat_response = ChatResponse {
            id: "test_id".to_string(),
            object: "chat.completion".to_string(),
            created: 1620000000,
            model: "gpt-4".to_string(),
            usage: usage.clone(),
            choices: vec![choice.clone()],
        };

        assert_eq!(chat_response.id, "test_id");
        assert_eq!(chat_response.object, "chat.completion");
        assert_eq!(chat_response.created, 1620000000);
        assert_eq!(chat_response.model, "gpt-4");
        assert_eq!(chat_response.usage, usage);
        assert_eq!(chat_response.choices.len(), 1);
        assert_eq!(chat_response.choices[0], choice);
    }

    #[test]
    fn test_usage_struct() {
        let usage = Usage {
            prompt_tokens: 10,
            completion_tokens: 20,
            total_tokens: 30,
        };

        assert_eq!(usage.prompt_tokens, 10);
        assert_eq!(usage.completion_tokens, 20);
        assert_eq!(usage.total_tokens, 30);
    }

    #[test]
    fn test_choice_struct() {
        let message = Message {
            role: Role::Assistant,
            content: "Sample response".to_string(),
        };

        let choice = Choice {
            message: message.clone(),
            finish_reason: "stop".to_string(),
        };

        assert_eq!(choice.message, message);
        assert_eq!(choice.finish_reason, "stop");
    }
}
